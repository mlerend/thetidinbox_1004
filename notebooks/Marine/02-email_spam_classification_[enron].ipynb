{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HYFlA7xMpKOB"
   },
   "source": [
    "# 02-email_spam_classification [enron]\n",
    "\n",
    "* __Status__ : OK\n",
    "* __Dataset__ : /thetidinbox_1004/notebooks/Marine/enron1.tar.gz [as an example]\n",
    "* __Source__ : Enron Email Dataset [https://www.cs.cmu.edu/~enron/]\n",
    "* __Labeled__ : Yes. Spam/Ham\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y2OQ9sk5pKOE"
   },
   "source": [
    "## 📚 **Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ToO-JplYpKOF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t-Sb0zckpKOQ"
   },
   "outputs": [],
   "source": [
    "emails_df = pd.read_csv('email_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject christma tree farm pictur</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject vastar resourc inc gari product high i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject calpin daili gas nomin calpin daili ga...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject issu fyi see note alreadi done stella ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject meter nov alloc fyi forward lauri alle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4989</th>\n",
       "      <td>subject pro forma invoic attach divid cover ga...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4990</th>\n",
       "      <td>subject str rndlen extra time word bodyhtml</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4991</th>\n",
       "      <td>subject check bb hey derm bbbbb check pari man...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992</th>\n",
       "      <td>subject hot job global market specialti po box...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4993</th>\n",
       "      <td>subject save ink ship cost save inkjet laser c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4994 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                message  class\n",
       "0                    subject christma tree farm pictur       0\n",
       "1     subject vastar resourc inc gari product high i...      0\n",
       "2     subject calpin daili gas nomin calpin daili ga...      0\n",
       "3     subject issu fyi see note alreadi done stella ...      0\n",
       "4     subject meter nov alloc fyi forward lauri alle...      0\n",
       "...                                                 ...    ...\n",
       "4989  subject pro forma invoic attach divid cover ga...      1\n",
       "4990       subject str rndlen extra time word bodyhtml       1\n",
       "4991  subject check bb hey derm bbbbb check pari man...      1\n",
       "4992  subject hot job global market specialti po box...      1\n",
       "4993  subject save ink ship cost save inkjet laser c...      1\n",
       "\n",
       "[4994 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yrnl30tupKPF"
   },
   "source": [
    "## Model setting\n",
    "\n",
    "Splitting the dataset into train and test data. The split is 70%-30% respectively.\n",
    "The splitting function is imported from the library 'sklearn'. this function has many parameter like features, target, test_size (this defines percentage of testset), shuffle (this defines if the data should be shuffled before splitting), random_state (this defines the seed used by random number generator) and stratify (data is split in stratified fashion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4mZvJrkspKPA"
   },
   "outputs": [],
   "source": [
    "# Define the independent variables as X\n",
    "X = emails_df['message'].values\n",
    "\n",
    "# Define the target as Y\n",
    "y = emails_df['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "06tBMdf_pKPF",
    "outputId": "bec4a653-da9b-4689-f00f-fca830ee4d2b"
   },
   "outputs": [],
   "source": [
    "# Create a train/test split using 30% test size.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, shuffle=True, random_state=123, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YGW7lExkpKPR"
   },
   "source": [
    "### Feature Extraction Process :\n",
    "\n",
    "Text data requires special preparation before you can start using it for predictive modeling. The text must be parsed to remove words, called tokenization. Then the words need to be encoded as integers or floating point values for use as input to a machine learning algorithm, called feature extraction (or vectorization).\n",
    "\n",
    "A simple and effective model for thinking about text documents in machine learning is called the Bag-of-Words Model, or BoW. This model first tokenizes the document into words and then assign these word a unique number. the fit() function in order to learn a vocabulary from one or more documents.\n",
    "\n",
    "Here, to convert into Bag of Words we use Count vectorizer. This provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6WklgZJepKPS",
    "outputId": "1f9b57df-c28f-4fbc-c583-e78e957485c4"
   },
   "outputs": [],
   "source": [
    "# Count Vectorizer/ Bag of Words\n",
    "\n",
    "# Creating  an instance of the CountVectorizer class\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "# The fit() function is to learn a vocabulary from one or more documents.\n",
    "bag_of_words_train = cv.fit_transform(X_train)\n",
    "# the transform() function is used to encode each document as a vector.\n",
    "bag_of_words_test = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oj0jfQ1ApKRG"
   },
   "source": [
    "### Model Selection:\n",
    "\n",
    "Applying 3 different classification models :\n",
    "1. MultinomialNB\n",
    "2. Logistic Regression \n",
    "3. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Accuracy: 0.977 (+/- 0.015)\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB(alpha=0.05)\n",
    "\n",
    "# Cross Validation using Multinomial NB\n",
    "scores_nb = cross_val_score(nb, bag_of_words_train, y_train, scoring= 'accuracy',cv=10)\n",
    "\n",
    "#The mean score and the 95% confidence interval of the score estimate are hence given by:\n",
    "print(\"MultinomialNB Accuracy: %0.3f (+/- %0.3f)\" % (scores_nb.mean(), scores_nb.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MultinomialNB': array([0.98      , 0.96857143, 0.96285714, 0.98      , 0.97714286,\n",
      "       0.97421203, 0.97421203, 0.99140401, 0.98567335, 0.97421203])}\n"
     ]
    }
   ],
   "source": [
    "accuracy_dict = {}\n",
    "accuracy_dict.update({'MultinomialNB':scores_nb})\n",
    "print(accuracy_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YSUKJuegpKRV",
    "outputId": "929810a0-ac37-4a19-a883-5c0ed94eeb10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression Accuracy: 0.976 (+/- 0.020)\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "\n",
    "# Cross Validation using Logistic Regression\n",
    "scores_logreg = cross_val_score(logreg, bag_of_words_train, y_train, scoring= 'accuracy',cv=10)\n",
    "\n",
    "#The mean score and the 95% confidence interval of the score estimate are hence given by:\n",
    "print(\"LogisticRegression Accuracy: %0.3f (+/- %0.3f)\" % (scores_logreg.mean(), scores_logreg.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1lVeAhqMpKRZ",
    "outputId": "a06af6e5-2a36-45a2-e466-5647c70c1a63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MultinomialNB': array([0.98      , 0.96857143, 0.96285714, 0.98      , 0.97714286,\n",
      "       0.97421203, 0.97421203, 0.99140401, 0.98567335, 0.97421203]), 'LogisticRegression': array([0.98      , 0.95714286, 0.97714286, 0.97142857, 0.98285714,\n",
      "       0.96275072, 0.97994269, 0.98853868, 0.98853868, 0.96848138])}\n"
     ]
    }
   ],
   "source": [
    "accuracy_dict.update({'LogisticRegression':scores_logreg})\n",
    "print(accuracy_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pg8Ms_C4pKRt",
    "outputId": "c9ab6f55-908b-42ad-d069-5ff8c705b910"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy: 0.87 (+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "# 10-fold cross-validation with K=5 for KNN (the n_neighbors parameter)\n",
    "# k = 5 for KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Use cross_val_score function\n",
    "# cv=10 for 10 folds\n",
    "scores_knn = cross_val_score(knn, bag_of_words_train, y_train, cv=10, scoring='accuracy')\n",
    "\n",
    "#The mean score and the 95% confidence interval of the score estimate are hence given by:\n",
    "print(\"KNN Accuracy: %0.2f (+/- %0.2f)\" % (scores_knn.mean(), scores_knn.std() * 2))\n",
    "\n",
    "predicted = cross_val_predict(knn, bag_of_words_train, y_train, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6S_Z5tRWpKRz",
    "outputId": "357f1f51-9361-4998-a4ab-421f50f06b8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MultinomialNB': array([0.98      , 0.96857143, 0.96285714, 0.98      , 0.97714286,\n",
      "       0.97421203, 0.97421203, 0.99140401, 0.98567335, 0.97421203]), 'LogisticRegression': array([0.98      , 0.95714286, 0.97714286, 0.97142857, 0.98285714,\n",
      "       0.96275072, 0.97994269, 0.98853868, 0.98853868, 0.96848138]), 'KNN': array([0.87428571, 0.85714286, 0.89142857, 0.84571429, 0.88571429,\n",
      "       0.86819484, 0.86246418, 0.86532951, 0.89684814, 0.85673352])}\n"
     ]
    }
   ],
   "source": [
    "accuracy_dict.update({'KNN':scores_knn})\n",
    "print(accuracy_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ndHvx6FopKR4",
    "outputId": "dfb6758a-c5d2-4edc-b157-8e96da197b82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MultinomialNB  LogisticRegression       KNN\n",
      "0       0.980000            0.980000  0.874286\n",
      "1       0.968571            0.957143  0.857143\n",
      "2       0.962857            0.977143  0.891429\n",
      "3       0.980000            0.971429  0.845714\n",
      "4       0.977143            0.982857  0.885714\n",
      "5       0.974212            0.962751  0.868195\n",
      "6       0.974212            0.979943  0.862464\n",
      "7       0.991404            0.988539  0.865330\n",
      "8       0.985673            0.988539  0.896848\n",
      "9       0.974212            0.968481  0.856734\n"
     ]
    }
   ],
   "source": [
    "accuracy_df = pd.DataFrame.from_dict(accuracy_dict,orient='columns')\n",
    "print(accuracy_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WpdqpOJypKSK"
   },
   "source": [
    "From the above dataframe we can infer that :\n",
    "* KNN showed the least accuracy \n",
    "* Multinomial Naive Bayes has scored highest accuracy of 97.7%\n",
    "* Thus for the further evaluation we may use Multinomial Naive Bayes Classifier\n",
    "\n",
    "### Model Evaluation :\n",
    "\n",
    "Proceeding further with Multinomial Naive Bayes Classifier.\n",
    "\n",
    "**Word Frequencies with TfidfVectorizer**: \n",
    "One issue with simple counts is that some words like “the” will appear many times and their large counts will not be very meaningful in the encoded vectors.\n",
    "An alternative is to calculate word frequencies, and by far the most popular method is called TF-IDF. This is an acronym than stands for “Term Frequency – Inverse Document” Frequency which are the components of the resulting scores assigned to each word. TF-IDF are word frequency scores that try to highlight words that are more interesting. The same create, fit, and transform process is used as with the CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dl1N-ZlnpKSP",
    "outputId": "22505ef6-f86c-4e64-e473-d8ff423db5e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Accuracy:  0.9874105865522175\n",
      "classify__alpha: 0.001\n",
      "tfidf__use_idf: True\n",
      "vectorize__ngram_range: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('vectorize', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('classify', MultinomialNB())])\n",
    "\n",
    "parameters = {'vectorize__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'classify__alpha':(1e-2, 1e-3)}\n",
    "\n",
    "gs_clf = GridSearchCV(pipe, parameters, cv=5, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(X_train, y_train)\n",
    "\n",
    "predict_MNB = gs_clf.predict(X_test)\n",
    "\n",
    "metrics.confusion_matrix(y_test, predict_MNB)\n",
    "\n",
    "print(\"MultinomialNB Accuracy: \",gs_clf.best_score_)\n",
    "for param_name in sorted(parameters.keys()):\n",
    "     print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PeL9TvtQpKSa"
   },
   "source": [
    "Displayed above is the Accuarcy of Naive Bayes Classifier after converting the texts into bag of words and then calculating its frequency and then finally applying the model to it.\n",
    "\n",
    "Accuracy is 98.74% .\n",
    "Also displayed is best parameters which helped in bringing out this precision to the model.\n",
    "\n",
    "*Now calculating the out-of-sample error:*\n",
    "\n",
    "Root Mean Square Error (RMSE) is the standard deviation of the residuals (prediction errors). Residuals are a measure of how far from the regression line data points are. RMSE is a measure of how spread out these residuals are. In other words, it tells you how concentrated the data is around the line of best fit..\n",
    "\n",
    "R-squared is a relative measure of fit, RMSE is an absolute measure of fit.\n",
    "\n",
    "Lower values of RMSE indicate better fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IVvgG-AhpKSb",
    "outputId": "1bda5465-9357-4e11-9b21-74e84d3a9924"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10003335000926467\n"
     ]
    }
   ],
   "source": [
    "rmse = sqrt(mean_squared_error(y_test, predict_MNB))\n",
    "'''returns out-of-sample error for already fit model.'''\n",
    "print(rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QLq-Vyn8pKSr"
   },
   "source": [
    "After final Prediction with an accuracy of 98.74% and RMSE of 0.1 shown above is the Confusion matrix of testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cJCPgTRjpKS5",
    "outputId": "4b3dfa24-71ec-40bf-fb13-d02d8549de15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9875864958954743"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate AUC\n",
    "metrics.roc_auc_score(y_test, predict_MNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N-JcvoA-pKTD"
   },
   "source": [
    "* AUC is useful as a single number summary of classifier performance\n",
    "* Higher value = better classifier"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Spam n Ham Detection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
