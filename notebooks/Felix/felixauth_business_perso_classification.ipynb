{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62220f50",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6bdf688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'provide,test'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [\"provide\",\"provide\",\"test\"]\n",
    "\",\".join(list(set(l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afb04eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "import re\n",
    "from string import punctuation\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "#nltk.download('words')\n",
    "import random\n",
    "import mailparser\n",
    "import requests\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report # for model evaluation metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24414c30",
   "metadata": {},
   "source": [
    "# Import Enron emails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22657af5",
   "metadata": {},
   "source": [
    "## Import originally labeled emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40c6f718",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_df = pd.DataFrame({\"message\" : [],\n",
    "                         \"category\" : []})\n",
    "for i in range(1,9):\n",
    "    path_data = f'../../raw_data/enron/emails/txt_files_categories/all_txt_files{i}.txt'\n",
    "    with open(path_data) as f:\n",
    "        contents = f.read()\n",
    "        emails_cat = [\"Message-ID: \" + email for email in contents.split(\"Message-ID: \") if email]\n",
    "        emails_df = pd.concat([emails_df, pd.DataFrame({\"message\" : emails_cat,\n",
    "                         \"category\" : int(i)})]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1db241",
   "metadata": {},
   "source": [
    "## Import model-labeled emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "462c6849",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_model_df = pd.read_csv(\"../../raw_data/new_emails_labeled.csv\")\n",
    "emails_model_df[\"category\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07037653",
   "metadata": {},
   "source": [
    "# Import conversational messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e446bb",
   "metadata": {},
   "source": [
    "## Conversational messages dataset n°1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ef182c",
   "metadata": {},
   "source": [
    "Source : https://github.com/alexa/Topical-Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05410bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Are you a fan of Google or Microsoft?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Both are excellent technology they are helpful...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm not  a huge fan of Google, but I use it a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Google provides online related services and pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yeah, their services are good. I'm just not a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body\n",
       "0              Are you a fan of Google or Microsoft?\n",
       "1  Both are excellent technology they are helpful...\n",
       "2  I'm not  a huge fan of Google, but I use it a ...\n",
       "3  Google provides online related services and pr...\n",
       "4  Yeah, their services are good. I'm just not a ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get('https://raw.githubusercontent.com/alexa/Topical-Chat/master/conversations/train.json').json()\n",
    "\n",
    "message_l = []\n",
    "for key in r.keys():\n",
    "    for message in r[key]['content']:\n",
    "        message_l.append(message[\"message\"])\n",
    "conv_message_one_df = pd.DataFrame({\"body\":message_l})\n",
    "conv_message_one_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2abdb8",
   "metadata": {},
   "source": [
    "Example :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0257d549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yeah that is for sure very different. even in the uk not all men could vote intul 1918. i wonder how one qualified'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_message_one_df.sample()[\"body\"].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb48a1b",
   "metadata": {},
   "source": [
    "## Conversational messages dataset n°2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50029804",
   "metadata": {},
   "source": [
    "Source : https://www.kaggle.com/datasets/thedevastator/unlock-the-creative-power-of-dynamic-dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95f8c278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i hate talking to people. i believe dragons ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wow, I am never shy. Do you have anxiety?\\n \"Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and why is that?\\n interesting but I know how ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"I think its because in my head, I think every...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i have three daughters. my wife and i like to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body\n",
       "0  i hate talking to people. i believe dragons ar...\n",
       "1  Wow, I am never shy. Do you have anxiety?\\n \"Y...\n",
       "2  and why is that?\\n interesting but I know how ...\n",
       "3  \"I think its because in my head, I think every...\n",
       "4  i have three daughters. my wife and i like to ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_train = f'../../raw_data/conv_messages_train.csv'\n",
    "path_test = f'../../raw_data/conv_messages_test.csv'\n",
    "conv_message_two_df_train = pd.read_csv(path_train)\n",
    "conv_message_two_df_test = pd.read_csv(path_test)\n",
    "conv_message_two_raw_df = pd.concat([conv_message_two_df_test, conv_message_two_df_train]).reset_index(drop=True)\n",
    "\n",
    "mess = []\n",
    "for i in range(0,len(conv_message_two_raw_df)):\n",
    "    mess.append(\"\".join(conv_message_two_raw_df.loc[i,\"personas\"]).replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\"))\n",
    "    mess.append(\"\".join(conv_message_two_raw_df.loc[i,\"previous_utterance\"]).replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\"))\n",
    "    mess.append(\"\".join(conv_message_two_raw_df.loc[i,\"free_messages\"]).replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\"))\n",
    "    mess.append(\"\".join(conv_message_two_raw_df.loc[i,\"guided_messages\"]).replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\"))\n",
    "conv_message_two_df = pd.DataFrame({\"body\":mess})\n",
    "conv_message_two_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b83700",
   "metadata": {},
   "source": [
    "Example :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ed61cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"I really dont get to see that kind of thing often since I just run a custom upholstery shop.\"\\n \"Really? Thats awesome. What was the question?\"\\n Cool. And what is the answer?\\n Oh wow. I hate the way that stuff sounds when my kids take their shoes off.\\n Hey, good for you!\\n \"One of them is six and the other is four. Its tough being a single mom.\"'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_message_two_df.sample()[\"body\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "384c5f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23196, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_message_two_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33acbdc",
   "metadata": {},
   "source": [
    "## Concatenating conversational messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d42adfc",
   "metadata": {},
   "source": [
    "Creating a dataset of **20 000** messages from conversational messages **n°1** and **20 000** from conversational messages **n°2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "423fbf49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 20000\n",
    "conv_message_df = pd.concat([conv_message_one_df.sample(n), conv_message_two_df.sample(n)]).reset_index(drop=True)\n",
    "conv_message_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5404c51",
   "metadata": {},
   "source": [
    "Example :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9168be8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it does but I am not really into all of that just the exercise part of it, not the spiritual\\n I read somewhere that it dates back to like the 6th or 5th centuries BCE in India.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_message_df.sample()[\"body\"].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f8b32f",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Testing other dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de7a5fcf",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Worried'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_data = '../../raw_data/Social Conversation (csv).csv'\n",
    "df_test = pd.read_csv(path_data, encoding=\"Latin-1\")\n",
    "df_test.sample()[\"??\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad2fbb4f",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UNCLASSIFIED\\nU.S. Department of State\\nCase No. F-2015-04841\\nDoc No. C05739608\\nDate: 05/13/2015\\nSTATE DEPT. - PRODUCED TO HOUSE SELECT BENGHAZI COMM.\\nSUBJECT TO AGREEMENT ON SENSITIVE INFORMATION & REDACTIONS. NO FOIA WAIVER.\\nRELEASE IN\\nPART B6\\nFrom: Sullivan, Jacobi <SullivanJJOstate.goy>\\nSent: Tuesday, March 29, 2011 10:41 PM\\nTo:\\nSubject: Fw: Libyan\\nFyi\\n-- Original Message â\\x80\\x94\\nFrom: Gordon, Philip\\nSent: Tuesday, March 29, 201105:36 PM\\nTo Rottman, Jeffrey D; Burns, William J; Sullivan, Jacob J\\nSubject: Re: Libyan\\now says the issue of the visitor is \"more complicated than they thought\" and he prefers to send me a secure\\nmessage in the morning. Will let you know.\\nMeanwhile, press here all about whether US/coalition going to warm rebels\" based on HRC comments about \"no\\ndecision yet, resolution allows that\". Tomorrow\\'s Guardian headline is \"Coalition ready to arm rebellion if Gadafy clings\\nto power\". Don\\'t know if playing same way back home.\\n--- Original Message ---\\nFrom: Feltmartâ\\x80\\x9e Jeffrey\\nSent: Tuesday, March 29, 2011 02:57 PM\\nTo: Gordon, Philip H; Burns, William J; Sullivan, Jacobi\\nSubject: Re: Libyan\\nThanks.\\nJeffrey Feltman\\nOriginal Message\\nFrom Gordon, Philip H\\nSent: Tuesday, March 29, 2011 02:34 PM\\nTo: Burns, William J; Feltman, Jeffrey D; Sullivan, Jacob 1\\nSubject: Libyan\\nays plane of Libyan mystery visitor just landed. Said he\\'d keep us posted as details emerge.\\nUNCLASSIFIED\\nU.S. Department of State\\nCase No. F-2015-04841\\nDoc No. C05739608\\nDate: 05/13/2015\\nSTATE DEPT. - PRODUCED TO HOUSE SELECT BENGHAZI COMM.\\nSUBJECT TO AGREEMENT ON SENSITIVE INFORMATION & REDACTIONS. NO FOIA WAIVER. STATE-SCB0045021\\nB6\\nB6\\n\\x0c'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_data = '../../raw_data/Emails.csv'\n",
    "df_test = pd.read_csv(path_data, encoding=\"Latin-1\")\n",
    "df_test.sample()[\"RawText\"].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd9c95d",
   "metadata": {},
   "source": [
    "# Parsing emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b095d49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date(email):\n",
    "    mail = mailparser.parse_from_string(email)\n",
    "    date = mail.date\n",
    "    return date\n",
    "\n",
    "def extract_sender(email):\n",
    "    mail = mailparser.parse_from_string(email)\n",
    "    if len(mail.from_) > 0:\n",
    "        sender = mail.from_[0][1]\n",
    "    else:\n",
    "        sender = mail.from_\n",
    "    return sender\n",
    "\n",
    "def extract_recipients(email):\n",
    "    mail = mailparser.parse_from_string(email)\n",
    "    if len(mail.to) > 0:\n",
    "        to = \",\".join([rec[1] for rec in mail.to])\n",
    "    else:\n",
    "        to = mail.to\n",
    "    return to\n",
    "\n",
    "def extract_header(email):\n",
    "    mail = mailparser.parse_from_string(email)\n",
    "    header = mail.subject\n",
    "    return header\n",
    "\n",
    "def extract_body(email):\n",
    "    mail = mailparser.parse_from_string(email)\n",
    "    body = mail.body\n",
    "    return body"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747387ce",
   "metadata": {},
   "source": [
    "**Parsing labeled emails**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90e792a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Email content 'mixed' not handled\n",
      "Email content 'mixed' not handled\n",
      "Email content 'alternative' not handled\n",
      "Email content 'alternative' not handled\n",
      "Email content 'mixed' not handled\n",
      "Email content 'mixed' not handled\n",
      "Email content 'mixed' not handled\n",
      "Email content 'mixed' not handled\n",
      "Email content 'alternative' not handled\n",
      "Email content 'alternative' not handled\n",
      "Email content 'mixed' not handled\n",
      "Email content 'mixed' not handled\n",
      "Email content 'mixed' not handled\n",
      "Email content 'mixed' not handled\n",
      "Email content 'alternative' not handled\n",
      "Email content 'alternative' not handled\n",
      "Email content 'mixed' not handled\n",
      "Email content 'mixed' not handled\n",
      "Email content 'mixed' not handled\n",
      "Email content 'mixed' not handled\n",
      "Email content 'alternative' not handled\n",
      "Email content 'alternative' not handled\n",
      "Email content 'mixed' not handled\n",
      "Email content 'mixed' not handled\n",
      "Email content 'mixed' not handled\n",
      "Email content 'mixed' not handled\n",
      "Email content 'alternative' not handled\n",
      "Email content 'alternative' not handled\n",
      "Email content 'mixed' not handled\n",
      "Email content 'mixed' not handled\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>date</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>header</th>\n",
       "      <th>body</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>197504</td>\n",
       "      <td>1999-10-18 08:47:00</td>\n",
       "      <td>steven.kean@enron.com</td>\n",
       "      <td>mark.schroeder@enron.com,kenneth.lay@enron.com...</td>\n",
       "      <td>Translation of articles</td>\n",
       "      <td>---------------------- Forwarded by Steven J K...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11991339</td>\n",
       "      <td>2001-03-05 16:23:00</td>\n",
       "      <td>drew.fossum@enron.com</td>\n",
       "      <td>darrell.schoolcraft@enron.com</td>\n",
       "      <td>TW Gas Sales: PRIVILEGED AND CONFIDENTIAL ATTO...</td>\n",
       "      <td>In anticipation of potential litigation involv...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7106753</td>\n",
       "      <td>2001-03-06 08:59:00</td>\n",
       "      <td>drew.fossum@enron.com</td>\n",
       "      <td>julia.white@enron.com,steven.january@enron.com</td>\n",
       "      <td>TW Gas Sales: PRIVILEGED AND CONFIDENTIAL ATTO...</td>\n",
       "      <td>Julia and Steve--here are some questions I've ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21267718</td>\n",
       "      <td>2001-03-06 19:59:00</td>\n",
       "      <td>drew.fossum@enron.com</td>\n",
       "      <td>julia.white@enron.com,steven.january@enron.com</td>\n",
       "      <td>TW Gas Sales: PRIVILEGED AND CONFIDENTIAL ATTO...</td>\n",
       "      <td>Julia and Steve--here are some questions I've ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20866019</td>\n",
       "      <td>2001-03-06 03:23:00</td>\n",
       "      <td>drew.fossum@enron.com</td>\n",
       "      <td>darrell.schoolcraft@enron.com</td>\n",
       "      <td>TW Gas Sales: PRIVILEGED AND CONFIDENTIAL ATTO...</td>\n",
       "      <td>In anticipation of potential litigation involv...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                date                   from  \\\n",
       "0    197504 1999-10-18 08:47:00  steven.kean@enron.com   \n",
       "1  11991339 2001-03-05 16:23:00  drew.fossum@enron.com   \n",
       "2   7106753 2001-03-06 08:59:00  drew.fossum@enron.com   \n",
       "3  21267718 2001-03-06 19:59:00  drew.fossum@enron.com   \n",
       "4  20866019 2001-03-06 03:23:00  drew.fossum@enron.com   \n",
       "\n",
       "                                                  to  \\\n",
       "0  mark.schroeder@enron.com,kenneth.lay@enron.com...   \n",
       "1                      darrell.schoolcraft@enron.com   \n",
       "2     julia.white@enron.com,steven.january@enron.com   \n",
       "3     julia.white@enron.com,steven.january@enron.com   \n",
       "4                      darrell.schoolcraft@enron.com   \n",
       "\n",
       "                                              header  \\\n",
       "0                            Translation of articles   \n",
       "1  TW Gas Sales: PRIVILEGED AND CONFIDENTIAL ATTO...   \n",
       "2  TW Gas Sales: PRIVILEGED AND CONFIDENTIAL ATTO...   \n",
       "3  TW Gas Sales: PRIVILEGED AND CONFIDENTIAL ATTO...   \n",
       "4  TW Gas Sales: PRIVILEGED AND CONFIDENTIAL ATTO...   \n",
       "\n",
       "                                                body  category  \n",
       "0  ---------------------- Forwarded by Steven J K...       1.0  \n",
       "1  In anticipation of potential litigation involv...       1.0  \n",
       "2  Julia and Steve--here are some questions I've ...       1.0  \n",
       "3  Julia and Steve--here are some questions I've ...       1.0  \n",
       "4  In anticipation of potential litigation involv...       1.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails_df_parsed = pd.DataFrame()\n",
    "emails_df_parsed[\"ID\"] = emails_df[\"message\"].apply(lambda x: re.search(r'\\d+',x)[0])\n",
    "emails_df_parsed[\"date\"] = emails_df[\"message\"].apply(extract_date)\n",
    "emails_df_parsed[\"from\"] = emails_df[\"message\"].apply(extract_sender)\n",
    "emails_df_parsed[\"to\"] = emails_df[\"message\"].apply(extract_recipients)\n",
    "emails_df_parsed[\"header\"] = emails_df[\"message\"].apply(extract_header)\n",
    "emails_df_parsed[\"body\"] = emails_df[\"message\"].apply(extract_body)\n",
    "emails_df_parsed[\"category\"] = emails_df[\"category\"]\n",
    "emails_df_parsed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048a219a",
   "metadata": {},
   "source": [
    "# EDA emails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fd80c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number of emails</th>\n",
       "      <th>% of total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>839.0</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>38.0</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>102.0</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>480.0</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>74.0</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>144.0</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.0</th>\n",
       "      <td>21.0</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>1716.0</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       number of emails  % of total\n",
       "1.0               839.0        0.49\n",
       "2.0                38.0        0.02\n",
       "3.0               102.0        0.06\n",
       "4.0               480.0        0.28\n",
       "5.0                74.0        0.04\n",
       "6.0               144.0        0.08\n",
       "7.0                21.0        0.01\n",
       "8.0                18.0        0.01\n",
       "Total            1716.0        0.99"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_email_per_cat = emails_df_parsed[\"category\"].value_counts().sort_index()\n",
    "weight_email_per_cat = emails_df_parsed[\"category\"].value_counts(normalize=True).sort_index()\n",
    "num_mail_df = pd.DataFrame({\"number of emails\" : num_email_per_cat.values,\n",
    "             \"% of total\" : round(weight_email_per_cat,2)}, index = num_email_per_cat.index)\n",
    "num_mail_df.loc[\"Total\"] = num_mail_df.sum()\n",
    "num_mail_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9141463",
   "metadata": {},
   "source": [
    "Keeping only **business-related** categories and attributing **category 0** for these emails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43ebfd43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>---------------------- Forwarded by Steven J K...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In anticipation of potential litigation involv...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Julia and Steve--here are some questions I've ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Julia and Steve--here are some questions I've ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In anticipation of potential litigation involv...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  category\n",
       "0  ---------------------- Forwarded by Steven J K...         0\n",
       "1  In anticipation of potential litigation involv...         0\n",
       "2  Julia and Steve--here are some questions I've ...         0\n",
       "3  Julia and Steve--here are some questions I've ...         0\n",
       "4  In anticipation of potential litigation involv...         0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails_business_df = emails_df_parsed[emails_df_parsed[\"category\"].isin([1,4,5,6])][[\"body\",\"category\"]]\n",
    "emails_business_df[\"category\"] = 0\n",
    "emails_business_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460768ef",
   "metadata": {},
   "source": [
    "# EDA conversational messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c452a998",
   "metadata": {},
   "source": [
    "Affecting **category 1** to conversational messages :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a4c1d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow that is a long time.  I had one for 3 year...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lol good plan, well watch out for the fly and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yeah and he must love to read since he spent s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I have been a big fan of the Patriots since Br...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>That must be how he developed his famous skyho...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  category\n",
       "0  Wow that is a long time.  I had one for 3 year...         1\n",
       "1  lol good plan, well watch out for the fly and ...         1\n",
       "2  Yeah and he must love to read since he spent s...         1\n",
       "3  I have been a big fan of the Patriots since Br...         1\n",
       "4  That must be how he developed his famous skyho...         1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_message_df[\"category\"] = 1\n",
    "conv_message_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e864708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average message length : 174.0 characters\n"
     ]
    }
   ],
   "source": [
    "length_message = [len(mess) for mess in conv_message_df[\"body\"]]\n",
    "print(\"Average message length :\", round(np.average(length_message),0), \"characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a68a296c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "length_message = [len(mess) for mess in conv_message_df[\"body\"]]\n",
    "plt.hist(length_message);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597b29cd",
   "metadata": {},
   "source": [
    "# Concatenating emails & messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e954729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39703</th>\n",
       "      <td>Rachel:\\n\\nMark Taylor will finalize all issue...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77913</th>\n",
       "      <td>i like to learn the history of the buildings.\\...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61203</th>\n",
       "      <td>I love google!  It's funny. The company starte...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60998</th>\n",
       "      <td>Nope. I never thought about it. Do you?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7546</th>\n",
       "      <td>EXECUTIVE SUMMARY\\n?\\tA New MOU Takes Shape in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    body  category\n",
       "39703  Rachel:\\n\\nMark Taylor will finalize all issue...         0\n",
       "77913  i like to learn the history of the buildings.\\...         1\n",
       "61203  I love google!  It's funny. The company starte...         1\n",
       "60998            Nope. I never thought about it. Do you?         1\n",
       "7546   EXECUTIVE SUMMARY\\n?\\tA New MOU Takes Shape in...         0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([emails_business_df,emails_model_df[[\"body\",\"category\"]],conv_message_df], axis=0).reset_index(drop=True)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f1fc4692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    41491\n",
       "1    40000\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c18e0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ARE YOU SINGLE?\\nVisit the web\\'s favorite meeting place.\\nThousands and thousands of people are developing quality relationships at this very moment...all from the comfort and safety of home.\\nClick Here to create a FREE profile on Dream Mates and meet that special someone right now!\\nhttp://www.play4keeps.com/rd.cgi?dreammates\\n\\n===========================================\\n\\nSign Up for our Games and Get $10 FREE!\\nhttp://www.play4keeps.com/rd.cgi?cashgames\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n<<<>>> <<<>>> <<<>>> <<<>>> <<<>>> <<<>>> <<<>>> <<<>>> <<<>>> <<<>>>\\n* To remove yourself from this mailing list, point your browser to:\\nhttp://i.pm0.net/remove?freebiecash:14\\n* Enter your email address (don.baughman@enron.com) in the field\\nprovided and click \"Unsubscribe\". The mailing list ID is \"freebiecash:14\".\\n\\nOR...\\n\\n* Reply to this message with the word \"remove\" in the subject line.\\n\\nThis message was sent to address don.baughman@enron.com\\nX-PMG-Recipient: don.baughman@enron.com\\n<<<>>> <<<>>> <<<>>> <<<>>> <<<>>> <<<>>> <<<>>> <<<>>> <<<>>> <<<>>>\\n\\n\\n\\n\\n\\npmguid:39.1759.9h08x'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails_model_df[[\"body\",\"category\"]].loc[1234,\"body\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "80c990bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"message_business_perso.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21f6306",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ce1258",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "18cbfb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_email(email):\n",
    "\n",
    "    # Remove mentions\n",
    "    email = re.sub(r'@\\w+', '', email)\n",
    "    # Remove urls\n",
    "    email = re.sub(r'http\\S+', ' ', email)\n",
    "    # Remove digits\n",
    "    email = re.sub(\"\\d+\", \" \", email)\n",
    "    # Remove backline character\n",
    "    email = email.replace('\\n', ' ')\n",
    "    # Remove forwarded emails\n",
    "#     email = re.sub(\"Forwarded by.*$\",' ', email)\n",
    "#     email = re.sub(\"Original Message.*$\",' ', email)\n",
    "    # Remove digits between brackets\n",
    "    email = re.sub(r'<.*>', '', email)\n",
    "    # Remove punctuations\n",
    "    email = email.translate(str.maketrans(\" \", \" \", punctuation))\n",
    "    email = email.lower()\n",
    "    # Remove some keyword\n",
    "    elements_to_drop = ['Message-ID:', 'Date:', 'From:', 'To:', 'Subject:', 'Cc:', 'Mime-Version:',\n",
    "     'Content-Type:', 'Content-Transfer-Encoding:', 'Bcc:', 'X-From:', 'X-To:', 'X-cc:', 'X-bcc:',\n",
    "     'X-Folder:', 'X-Origin:', 'X-FileName:', 'cc', '\\t', '--', 'Sent', ' --', '-', '/', '\\n', 'Re:', 'FW:']\n",
    "    for element in elements_to_drop:\n",
    "        email = email.replace(element, ' ')\n",
    "\n",
    "    return email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39138336",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.loc[df[\"body\"].notna()]\n",
    "df_cleaned[\"body\"] = df_cleaned[\"body\"].apply(clean_email)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4077bb5",
   "metadata": {},
   "source": [
    "## Stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4613701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopword_removal(email):\n",
    "\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "\n",
    "    email = email.split()\n",
    "    filtered_sentence = \"\"\n",
    "\n",
    "    for w in email: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence = filtered_sentence + w +\" \"\n",
    "\n",
    "    return filtered_sentence\n",
    "\n",
    "def unknown_word(email):\n",
    "    \n",
    "    words = set(nltk.corpus.words.words())\n",
    "\n",
    "    filtered_sentence = \" \".join(w for w in nltk.wordpunct_tokenize(sent) if w.lower() in words or not w.isalpha())\n",
    "    \n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e362a2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned[\"body\"] = df_cleaned[\"body\"].apply(stopword_removal)\n",
    "# df_cleaned[\"body\"] = df_cleaned[\"body\"].apply(unknown_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa93f6f",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bf4957ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned[\"body\"] = df_cleaned[\"body\"].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c532168d",
   "metadata": {},
   "source": [
    "## Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c1652c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizing(email):\n",
    "    \n",
    "    # 1 - Lemmatizing the verbs\n",
    "    verb_lemmatized = [                  \n",
    "    WordNetLemmatizer().lemmatize(word, pos = \"v\") # v --> verbs\n",
    "    for word in email   \n",
    "    ]\n",
    "\n",
    "    # 2 - Lemmatizing the nouns\n",
    "    noun_lemmatized = [                 \n",
    "    WordNetLemmatizer().lemmatize(word, pos = \"n\") # n --> nouns\n",
    "    for word in verb_lemmatized\n",
    "    ]\n",
    "    \n",
    "    return noun_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7d6a3cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned[\"body\"] = df_cleaned[\"body\"].apply(lemmatizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8cb410ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned[df_cleaned[\"body\"].map(lambda d: len(d)) > 0]\n",
    "df_cleaned[\"body\"] = df_cleaned[\"body\"].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3a0291",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ac5956",
   "metadata": {},
   "source": [
    "### Selecting random message number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b71354ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25715"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_mess = random.randint(0,len(df_cleaned))\n",
    "random_mess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88adc98",
   "metadata": {},
   "source": [
    "### Message before preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "23e06424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sara Sandy\\n903 w Glendale 4\\nPhoenix, AZ 85021\\nSarasandy@aol.com\\n\\nTo Mr. Ken Lay,\\n\\nI'm writing to urge you to donate the millions of dollars you made from selling Enron stock before the company declared bankruptcy to funds, such as Enron Employee Transition Fund and REACH, that benefit the company's employees, who lost their retirement savings, and provide relief to low-income consumers in California, who can't afford to pay their energy bills.  Enron and you made millions out of the pocketbooks of California consumers and from the efforts of your employees.\\n\\nIndeed, while you netted well over a $100 million, many of Enron's employees were financially devastated when the company declared bankruptcy and their retirement plans were wiped out.  And Enron made an astronomical profit during the California energy crisis last year.  As a result, there are thousands of consumers who are unable to pay their basic energy bills and the largest utility in the state is bankrupt.\\n\\nThe New York Times reported that you sold $101 million worth of Enron stock while aggressively urging the company's employees to keep buying it.  Please donate this money to the funds set up to help repair the lives of those Americans hurt by Enron's underhanded dealings.\\n\\nSincerely,\\n\\nSara Sandy\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[random_mess,\"body\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cd81a8",
   "metadata": {},
   "source": [
    "### Message after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "65adeb42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sara sandy w glendale phoenix az sarasandycom mr ken lay im write urge donate million dollar make sell enron stock company declare bankruptcy fund enron employee transition fund reach benefit company employee lose retirement save provide relief lowincome consumer california cant afford pay energy bill enron make million pocketbook california consumer effort employee indeed net well million many enrons employee financially devastate company declare bankruptcy retirement plan wipe enron make astronomical profit california energy crisis last year result thousand consumer unable pay basic energy bill largest utility state bankrupt new york time report sell million worth enron stock aggressively urge company employee keep buy please donate money fund set help repair live american hurt enrons underhanded deal sincerely sara sandy'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.loc[random_mess,\"body\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4977c57",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "de5e3b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.99      0.91      8345\n",
      "           1       0.99      0.79      0.88      7936\n",
      "\n",
      "    accuracy                           0.90     16281\n",
      "   macro avg       0.91      0.89      0.89     16281\n",
      "weighted avg       0.91      0.90      0.90     16281\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_cleaned[\"body\"], df_cleaned[\"category\"], test_size=0.2)\n",
    "\n",
    "# Vectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=0.1,max_df=0.7)\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "X_test_counts = vectorizer.transform(X_test)\n",
    "\n",
    "# Model\n",
    "model = MultinomialNB(alpha=0.01)\n",
    "model.fit(X_train_counts, y_train)\n",
    "y_pred = model.predict(X_test_counts)\n",
    "\n",
    "recall = recall_score(y_test, y_pred, average=None)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(classification_report(y_test, model.predict(X_test_counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbff24a",
   "metadata": {},
   "source": [
    "## Testing the prediction on the \"purely personal\" emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "085ab5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only category 4 (personal emails)\n",
    "emails_perso = emails_df_parsed[emails_df_parsed[\"category\"]==2]\n",
    "# preprocessing\n",
    "emails_perso_cleaned = emails_perso.loc[emails_perso[\"body\"].notna()]\n",
    "emails_perso_cleaned[\"body\"] = emails_perso_cleaned[\"body\"].apply(clean_email)\n",
    "emails_perso_cleaned[\"body\"] = emails_perso_cleaned[\"body\"].apply(stopword_removal)\n",
    "emails_perso_cleaned[\"body\"] = emails_perso_cleaned[\"body\"].apply(word_tokenize)\n",
    "emails_perso_cleaned[\"body\"] = emails_perso_cleaned[\"body\"].apply(lemmatizing)\n",
    "emails_perso_cleaned = emails_perso_cleaned[emails_perso_cleaned[\"body\"].map(lambda d: len(d)) > 0]\n",
    "emails_perso_cleaned[\"body\"] = emails_perso_cleaned[\"body\"].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "298b39ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>category_predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>advice much esteem spouse would like express g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>forward steven j keanhouees pm richard shapiro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>danastaubcom jamesdrummeycom richardhorlbeckco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>baby im go san francisco aug sunday return wed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>please forward sue walden get idea forward ste...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hey youre talk future president soooo look for...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tony pryor gpg legal ask credentialstickets le...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hardly believe peggy noonan write im sucker bl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>test note capability enron home computer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>enjoy forward steven j keannaenron pm enron ca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>september mimeversion contenttype multipartmix...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>love baby study hard keannora subject hi daddy...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>thank invite follow voicemail id love see gene...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>objection robert kean subject phil karen kathy...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>congratulation appointment fox thats great new...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ill give enron capital trade resource corp jef...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>forward steven j keannaenron enron capital tra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>forward steven j keannaenron enron capital tra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>im check melissa love talk houston real estate...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>love call</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>george herbert hoover bush robert kean subject...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>sorry guy give really good life robert kean su...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>yes phillippe leave join firm new york dont kn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>thank youi think janemtholtcom skeancom subjec...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>love call</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>george herbert hoover bush robert kean subject...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>sorry guy give really good life robert kean su...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>love availability beautiful lass view thing in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>jennifer thankyou step guide process forward s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>image qwiklist mya ount rental history ship li...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>kevin thank become old man vince original mess...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>bill thank forward message son vince original ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>hi julie rice ask teach another course energy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>fyi dad original message lawrencelrtnmtcom mai...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>larry thank original message lawrencelrtnmtcom...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>original message crenshaw shirley send friday ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>ill give enron capital trade resource corp jef...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>jeff think might find interest would guess cal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 body  category_predict\n",
       "0   advice much esteem spouse would like express g...                 0\n",
       "1   forward steven j keanhouees pm richard shapiro...                 0\n",
       "2   danastaubcom jamesdrummeycom richardhorlbeckco...                 0\n",
       "3   baby im go san francisco aug sunday return wed...                 1\n",
       "4   please forward sue walden get idea forward ste...                 0\n",
       "5   hey youre talk future president soooo look for...                 0\n",
       "6   tony pryor gpg legal ask credentialstickets le...                 1\n",
       "7   hardly believe peggy noonan write im sucker bl...                 0\n",
       "8            test note capability enron home computer                 0\n",
       "9   enjoy forward steven j keannaenron pm enron ca...                 0\n",
       "10  september mimeversion contenttype multipartmix...                 1\n",
       "11  love baby study hard keannora subject hi daddy...                 0\n",
       "12  thank invite follow voicemail id love see gene...                 0\n",
       "13  objection robert kean subject phil karen kathy...                 0\n",
       "14  congratulation appointment fox thats great new...                 0\n",
       "15  ill give enron capital trade resource corp jef...                 0\n",
       "16  forward steven j keannaenron enron capital tra...                 0\n",
       "17  forward steven j keannaenron enron capital tra...                 0\n",
       "18  im check melissa love talk houston real estate...                 0\n",
       "19                                          love call                 0\n",
       "20  george herbert hoover bush robert kean subject...                 1\n",
       "21  sorry guy give really good life robert kean su...                 0\n",
       "22  yes phillippe leave join firm new york dont kn...                 0\n",
       "23  thank youi think janemtholtcom skeancom subjec...                 0\n",
       "24                                          love call                 0\n",
       "25  george herbert hoover bush robert kean subject...                 1\n",
       "26  sorry guy give really good life robert kean su...                 0\n",
       "27  love availability beautiful lass view thing in...                 0\n",
       "28  jennifer thankyou step guide process forward s...                 0\n",
       "29  image qwiklist mya ount rental history ship li...                 0\n",
       "30  kevin thank become old man vince original mess...                 0\n",
       "31  bill thank forward message son vince original ...                 0\n",
       "32  hi julie rice ask teach another course energy ...                 0\n",
       "33  fyi dad original message lawrencelrtnmtcom mai...                 0\n",
       "34  larry thank original message lawrencelrtnmtcom...                 0\n",
       "35  original message crenshaw shirley send friday ...                 0\n",
       "36  ill give enron capital trade resource corp jef...                 0\n",
       "37  jeff think might find interest would guess cal...                 0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_perso = emails_perso_cleaned[\"body\"].reset_index(drop=True)\n",
    "X_perso_vec = vectorizer.transform(X_perso)\n",
    "y_pred_perso = model.predict(X_perso_vec)\n",
    "df_pred = pd.DataFrame({\"body\" : X_perso.values,\n",
    "             \"category_predict\" : y_pred_perso})\n",
    "df_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0889859e",
   "metadata": {},
   "source": [
    "**Personal test** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1ebaacae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"schedule a review\"\n",
    "text = clean_email(text)\n",
    "text = stopword_removal(text)\n",
    "text = word_tokenize(text)\n",
    "text = lemmatizing(text)\n",
    "text = \" \".join(text)\n",
    "text_vec = vectorizer.transform(pd.Series(text))\n",
    "y_pred_text = model.predict(text_vec)\n",
    "y_pred_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e417f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
